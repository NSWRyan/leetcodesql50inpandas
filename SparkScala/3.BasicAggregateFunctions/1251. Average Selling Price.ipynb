{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SLF4J: No SLF4J providers were found.\n",
                        "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
                        "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
                        "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Spark is ready!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
                            "\u001b[39m\n",
                            "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
                            "\u001b[39m\n",
                            "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
                            "\n",
                            "\u001b[39m\n",
                            "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@e48186f"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import $ivy.`org.apache.spark:spark-sql_2.13:3.3.2`\n",
                "import org.apache.spark.sql._\n",
                "import org.apache.spark.sql.functions._\n",
                "\n",
                "val spark = SparkSession.builder()\n",
                "  .appName(\"SparkTest\")\n",
                "  .master(\"local[*]\")\n",
                "  .getOrCreate()\n",
                "spark.sparkContext.setLogLevel(\"ERROR\")\n",
                "  \n",
                "// http://localhost:4040/jobs/\n",
                "println(\"Spark is ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------+----------+----------+-----+\n",
                        "|product_id|start_date|  end_date|price|\n",
                        "+----------+----------+----------+-----+\n",
                        "|         1|2019-02-17|2019-02-28|    5|\n",
                        "|         1|2019-03-01|2019-03-22|   20|\n",
                        "|         2|2019-02-01|2019-02-20|   15|\n",
                        "|         2|2019-02-21|2019-03-31|   30|\n",
                        "+----------+----------+----------+-----+\n",
                        "\n",
                        "+----------+-------------+-----+\n",
                        "|product_id|purchase_date|units|\n",
                        "+----------+-------------+-----+\n",
                        "|         1|   2019-02-25|  100|\n",
                        "|         1|   2019-03-01|   15|\n",
                        "|         2|   2019-02-10|  200|\n",
                        "|         2|   2019-03-22|   30|\n",
                        "+----------+-------------+-----+\n",
                        "\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
                            "// Data\n",
                            "\u001b[39m\n",
                            "defined \u001b[32mclass\u001b[39m \u001b[36mPrices\u001b[39m\n",
                            "defined \u001b[32mclass\u001b[39m \u001b[36mUnitsSold\u001b[39m\n",
                            "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
                            "  (\u001b[32m1\u001b[39m, \u001b[32m\"2019-02-17\"\u001b[39m, \u001b[32m\"2019-02-28\"\u001b[39m, \u001b[32m5\u001b[39m),\n",
                            "  (\u001b[32m1\u001b[39m, \u001b[32m\"2019-03-01\"\u001b[39m, \u001b[32m\"2019-03-22\"\u001b[39m, \u001b[32m20\u001b[39m),\n",
                            "  (\u001b[32m2\u001b[39m, \u001b[32m\"2019-02-01\"\u001b[39m, \u001b[32m\"2019-02-20\"\u001b[39m, \u001b[32m15\u001b[39m),\n",
                            "  (\u001b[32m2\u001b[39m, \u001b[32m\"2019-02-21\"\u001b[39m, \u001b[32m\"2019-03-31\"\u001b[39m, \u001b[32m30\u001b[39m)\n",
                            ")\n",
                            "\u001b[36mdata2\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
                            "  (\u001b[32m1\u001b[39m, \u001b[32m\"2019-02-25\"\u001b[39m, \u001b[32m100\u001b[39m),\n",
                            "  (\u001b[32m1\u001b[39m, \u001b[32m\"2019-03-01\"\u001b[39m, \u001b[32m15\u001b[39m),\n",
                            "  (\u001b[32m2\u001b[39m, \u001b[32m\"2019-02-10\"\u001b[39m, \u001b[32m200\u001b[39m),\n",
                            "  (\u001b[32m2\u001b[39m, \u001b[32m\"2019-03-22\"\u001b[39m, \u001b[32m30\u001b[39m)\n",
                            ")\n",
                            "\u001b[36mpricesDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPrices\u001b[39m] = [product_id: int, start_date: date ... 2 more fields]\n",
                            "\u001b[36munitsSoldDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mUnitsSold\u001b[39m] = [product_id: int, purchase_date: date ... 1 more field]"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import spark.implicits._\n",
                "// Data\n",
                "case class Prices(product_id: Int, start_date: java.sql.Date, end_date: java.sql.Date, price: Int)\n",
                "case class UnitsSold(product_id: Int, purchase_date: java.sql.Date, units: Int)\n",
                "val data = Seq[(Int, String, String, Int)](\n",
                "  (1, \"2019-02-17\", \"2019-02-28\", 5), \n",
                "  (1, \"2019-03-01\", \"2019-03-22\", 20), \n",
                "  (2, \"2019-02-01\", \"2019-02-20\", 15), \n",
                "  (2, \"2019-02-21\", \"2019-03-31\", 30)\n",
                ")\n",
                "val data2 = Seq[(Int, String, Int)](\n",
                "  (1, \"2019-02-25\", 100), \n",
                "  (1, \"2019-03-01\", 15), \n",
                "  (2, \"2019-02-10\", 200), \n",
                "  (2, \"2019-03-22\", 30)\n",
                ")\n",
                "\n",
                "\n",
                "val pricesDS: Dataset[Prices] = data.toDF(\"product_id\", \"start_date\", \"end_date\", \"price\")\n",
                "  .withColumn(\"start_date\", to_date(col(\"start_date\")))\n",
                "  .withColumn(\"end_date\", to_date(col(\"end_date\")))\n",
                "  .as[Prices]\n",
                "val unitsSoldDS: Dataset[UnitsSold] = data2.toDF(\"product_id\", \"purchase_date\", \"units\")\n",
                "  .withColumn(\"purchase_date\", to_date(col(\"purchase_date\")))\n",
                "  .as[UnitsSold]\n",
                "\n",
                "pricesDS.show()\n",
                "unitsSoldDS.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Write a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places. If a product does not have any sold units, its average selling price is assumed to be 0.\n",
                "\n",
                "Return the result table in any order.\n",
                "\n",
                "The result format is in the following example.\n",
                "\n",
                "| product_id | average_price |\n",
                "|------------|---------------|\n",
                "| 1          | 6.96          |\n",
                "| 2          | 16.96         |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+----------+-------------+\n",
                        "|product_id|average_price|\n",
                        "+----------+-------------+\n",
                        "|         1|         6.96|\n",
                        "|         2|        16.96|\n",
                        "+----------+-------------+\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "pricesDS.join(\n",
                "        unitsSoldDS, \n",
                "        (unitsSoldDS(\"product_id\") === pricesDS(\"product_id\")) &&\n",
                "        (unitsSoldDS(\"purchase_date\") >= pricesDS(\"start_date\")) &&\n",
                "        (unitsSoldDS(\"purchase_date\") <= pricesDS(\"end_date\")), \n",
                "        \"inner\"\n",
                "    )\n",
                "    .select(pricesDS(\"product_id\").as(\"product_id\"), col(\"price\"), col(\"units\"))\n",
                "    .groupBy(col(\"product_id\"))\n",
                "    .agg(round(sum(col(\"price\") * col(\"units\"))/sum(col(\"units\")), 2).as(\"average_price\"))\n",
                "    .show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "spark.stop"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Scala (2.13.10)",
            "language": "scala",
            "name": "scala213"
        },
        "language_info": {
            "codemirror_mode": "text/x-scala",
            "file_extension": ".sc",
            "mimetype": "text/x-scala",
            "name": "scala",
            "nbconvert_exporter": "script",
            "version": "2.13.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
