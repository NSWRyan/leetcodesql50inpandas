{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2c395174"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.3.2`\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkTest\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mMovie\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mUser\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mMovieRating\u001b[39m\n",
       "\u001b[36mmoviesData\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m\"Avengers\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m\"Frozen 2\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m\"Joker\"\u001b[39m)\n",
       ")\n",
       "\u001b[36musersData\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m\"Daniel\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m\"Monica\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m\"Maria\"\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m\"James\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mmovieRatingData\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m\"2020-01-12\"\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m\"2020-02-11\"\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m\"2020-02-12\"\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m\"2020-01-01\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m\"2020-02-17\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m\"2020-02-01\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m\"2020-03-01\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m\"2020-02-22\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m\"2020-02-25\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mmoviesDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mMovie\u001b[39m] = [movie_id: int, title: string]\n",
       "\u001b[36musersDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mUser\u001b[39m] = [user_id: int, name: string]\n",
       "\u001b[36mmovieRatingDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mMovieRating\u001b[39m] = [movie_id: int, user_id: int ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "case class Movie(movie_id: Int, title: String)\n",
    "case class User(user_id: Int, name: String)\n",
    "case class MovieRating(movie_id: Int, user_id: Int, rating: Int, created_at: java.sql.Date)\n",
    "\n",
    "val moviesData: Seq[(Int, String)] = Seq(\n",
    "  (1, \"Avengers\"),\n",
    "  (2, \"Frozen 2\"),\n",
    "  (3, \"Joker\")\n",
    ")\n",
    "\n",
    "val usersData: Seq[(Int, String)] = Seq(\n",
    "  (1, \"Daniel\"),\n",
    "  (2, \"Monica\"),\n",
    "  (3, \"Maria\"),\n",
    "  (4, \"James\")\n",
    ")\n",
    "\n",
    "val movieRatingData: Seq[(Int, Int, Int, String)] = Seq(\n",
    "  (1, 1, 3, \"2020-01-12\"),\n",
    "  (1, 2, 4, \"2020-02-11\"),\n",
    "  (1, 3, 2, \"2020-02-12\"),\n",
    "  (1, 4, 1, \"2020-01-01\"),\n",
    "  (2, 1, 5, \"2020-02-17\"),\n",
    "  (2, 2, 2, \"2020-02-01\"),\n",
    "  (2, 3, 2, \"2020-03-01\"),\n",
    "  (3, 1, 3, \"2020-02-22\"),\n",
    "  (3, 2, 4, \"2020-02-25\")\n",
    ")\n",
    "\n",
    "val moviesDS = moviesData.toDF(\"movie_id\", \"title\").as[Movie]\n",
    "val usersDS = usersData.toDF(\"user_id\", \"name\").as[User]\n",
    "val movieRatingDS = movieRatingData.toDF(\"movie_id\", \"user_id\", \"rating\", \"created_at\")\n",
    "  .withColumn(\"created_at\", col(\"created_at\").cast(\"date\"))\n",
    "  .as[MovieRating]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a solution to:\n",
    "\n",
    "* Find the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.\n",
    "* Find the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "| results      |\n",
    "|--------------|\n",
    "| Daniel       |\n",
    "| Frozen 2     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+\n",
      "|movie_id|user_id|rating|created_at|\n",
      "+--------+-------+------+----------+\n",
      "|       1|      1|     3|2020-01-12|\n",
      "|       1|      2|     4|2020-02-11|\n",
      "|       1|      3|     2|2020-02-12|\n",
      "|       1|      4|     1|2020-01-01|\n",
      "|       2|      1|     5|2020-02-17|\n",
      "|       2|      2|     2|2020-02-01|\n",
      "|       2|      3|     2|2020-03-01|\n",
      "|       3|      1|     3|2020-02-22|\n",
      "|       3|      2|     4|2020-02-25|\n",
      "+--------+-------+------+----------+\n",
      "\n",
      "+--------+\n",
      "| results|\n",
      "+--------+\n",
      "|  Daniel|\n",
      "|Frozen 2|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@eae4923\n",
       "\u001b[36mtopUserDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [results: string]\n",
       "\u001b[36mwindowSpec2\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@80d5523\n",
       "\u001b[36mtopMovieDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [results: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieRatingDS.show()\n",
    "val windowSpec = Window.orderBy(col(\"cnt\").desc)\n",
    "val topUserDF = movieRatingDS\n",
    "    .groupBy(col(\"user_id\"))\n",
    "    .agg(count(lit(1)).as(\"cnt\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "    .filter(col(\"rank\") === 1)\n",
    "    .select(col(\"user_id\"))\n",
    "    .join(usersDS, Seq(\"user_id\"), \"left_outer\")\n",
    "    .sort(col(\"name\").asc)\n",
    "    .select(col(\"name\").as(\"results\"))\n",
    "    .limit(1)\n",
    "\n",
    "val windowSpec2 = Window.orderBy(col(\"avg\").desc)\n",
    "\n",
    "val topMovieDF = movieRatingDS\n",
    "    .filter(date_trunc(\"month\", col(\"created_at\")) === to_date(lit(\"2020-02-01\")))\n",
    "    .groupBy(col(\"movie_id\"))\n",
    "    .agg(avg(col(\"rating\")).as(\"avg\"))\n",
    "    .withColumn(\"rank\", dense_rank().over(windowSpec2))\n",
    "    .filter(col(\"rank\") === 1)\n",
    "    .select(col(\"movie_id\"))\n",
    "    .join(moviesDS, Seq(\"movie_id\"), \"left_outer\")\n",
    "    .sort(col(\"title\").asc)\n",
    "    .select(col(\"title\").as(\"results\"))\n",
    "    .limit(1)\n",
    "\n",
    "topUserDF.union(topMovieDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.13.10)",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
