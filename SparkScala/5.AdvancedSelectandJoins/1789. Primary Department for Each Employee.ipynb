{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7ec1b4cb"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.3.2`\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkTest\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mEmployeeDepartment\u001b[39m\n",
       "\u001b[36memployeeDepartment\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m\"N\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m\"Y\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m\"N\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m\"N\"\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m\"N\"\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m\"Y\"\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m\"N\"\u001b[39m)\n",
       ")\n",
       "\u001b[36memployeeDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mEmployeeDepartment\u001b[39m] = [employee_id: int, department_id: int ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "case class EmployeeDepartment(employee_id: Int, department_id: Int, primary_flag: String)\n",
    "\n",
    "val employeeDepartment: Seq[(Int, Int, String)] = Seq(\n",
    "  (1, 1, \"N\"),\n",
    "  (2, 1, \"Y\"),\n",
    "  (2, 2, \"N\"),\n",
    "  (3, 3, \"N\"),\n",
    "  (4, 2, \"N\"),\n",
    "  (4, 3, \"Y\"),\n",
    "  (4, 4, \"N\")\n",
    ")\n",
    "\n",
    "val employeeDS = employeeDepartment.toDF(\"employee_id\", \"department_id\", \"primary_flag\").as[EmployeeDepartment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employees can belong to multiple departments. When the employee joins other departments, they need to decide which department is their primary department. Note that when an employee belongs to only one department, their primary column is 'N'.\n",
    "\n",
    "Write a solution to report all the employees with their primary department. For employees who belong to one department, report their only department.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "| employee_id | department_id |\n",
    "|-------------|---------------|\n",
    "| 1           | 1             |\n",
    "| 2           | 1             |\n",
    "| 3           | 3             |\n",
    "| 4           | 3             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            3|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeeDS.groupByKey(row => row.employee_id)\n",
    "    .mapGroups{\n",
    "        (employee_id: Int, iter: Iterator[EmployeeDepartment]) => \n",
    "        val list = iter.toList\n",
    "        if(list.length == 1) {\n",
    "            list.apply(0)\n",
    "        } else {\n",
    "            list.filter(x => {x.primary_flag == \"Y\"}).apply(0)\n",
    "        }\n",
    "    }.drop(\"primary_flag\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|employee_id|department_id|\n",
      "+-----------+-------------+\n",
      "|          1|            1|\n",
      "|          2|            1|\n",
      "|          3|            3|\n",
      "|          4|            3|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@2f78f3b7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window.partitionBy(col(\"employee_id\"))\n",
    "    .orderBy(col(\"primary_flag\").desc)\n",
    "\n",
    "employeeDS.withColumn(\"rownum\", row_number().over(windowSpec))\n",
    "    .filter(col(\"rownum\") === 1)\n",
    "    .drop(\"primary_flag\", \"rownum\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.13.10)",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
