{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@40b33105"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.3.2`\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkTest\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mQueuePerson\u001b[39m\n",
       "\u001b[36mqueuePerson\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m5\u001b[39m, \u001b[32m\"Alice\"\u001b[39m, \u001b[32m250\u001b[39m, \u001b[32m1\u001b[39m),\n",
       "  (\u001b[32m4\u001b[39m, \u001b[32m\"Bob\"\u001b[39m, \u001b[32m175\u001b[39m, \u001b[32m5\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m\"Alex\"\u001b[39m, \u001b[32m350\u001b[39m, \u001b[32m2\u001b[39m),\n",
       "  (\u001b[32m6\u001b[39m, \u001b[32m\"John Cena\"\u001b[39m, \u001b[32m400\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m\"Winston\"\u001b[39m, \u001b[32m500\u001b[39m, \u001b[32m6\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m\"Marie\"\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m4\u001b[39m)\n",
       ")\n",
       "\u001b[36mqueueDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mQueuePerson\u001b[39m] = [person_id: int, person_name: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "case class QueuePerson(person_id: Int, person_name: String, weight: Int, turn: Int)\n",
    "\n",
    "val queuePerson: Seq[(Int, String, Int, Int)] = Seq(\n",
    "  (5, \"Alice\", 250, 1),\n",
    "  (4, \"Bob\", 175, 5),\n",
    "  (3, \"Alex\", 350, 2),\n",
    "  (6, \"John Cena\", 400, 3),\n",
    "  (1, \"Winston\", 500, 6),\n",
    "  (2, \"Marie\", 200, 4)\n",
    ")\n",
    "\n",
    "val queueDS = queuePerson.toDF(\"person_id\", \"person_name\", \"weight\", \"turn\").as[QueuePerson]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board.\n",
    "\n",
    "Write a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.\n",
    "\n",
    "Note that only one person can board the bus at any given turn.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "\n",
    "| person_name |\n",
    "|-------------|\n",
    "| John Cena   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@47d917fa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window.orderBy(col(\"turn\"))\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|person_name|\n",
      "+-----------+\n",
      "|  John Cena|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msumDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [person_id: int, person_name: string ... 3 more fields]\n",
       "\u001b[36mlastWeightDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [sum: bigint]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sumDF = queueDS.withColumn(\"sum\", sum(col(\"weight\")).over(windowSpec))\n",
    "    .filter(col(\"sum\") <= 1000)\n",
    "val lastWeightDF = sumDF.select(\"sum\").agg(max(col(\"sum\")).as(\"sum\"))\n",
    "sumDF.join(lastWeightDF, Seq(\"sum\"), \"inner\")\n",
    "    .select(\"person_name\")\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.13.10)",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
