{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@418bce25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.3.2`\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkTest\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "  \n",
    "// http://localhost:4040/jobs/\n",
    "println(\"Spark is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mPerson\u001b[39m\n",
       "\u001b[36mperson\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m\"john@example.com\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m\"bob@example.com\"\u001b[39m),\n",
       "  (\u001b[32m3\u001b[39m, \u001b[32m\"john@example.com\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mpersonDS\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mPerson\u001b[39m] = [id: int, email: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "case class Person(id: Int, email: String)\n",
    "\n",
    "val person: Seq[(Int, String)] = Seq(\n",
    "  (1, \"john@example.com\"),\n",
    "  (2, \"bob@example.com\"),\n",
    "  (3, \"john@example.com\")\n",
    ")\n",
    "\n",
    "val personDS = person.toDF(\"id\", \"email\").as[Person]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.\n",
    "\n",
    "For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.\n",
    "\n",
    "For Pandas users, please note that you are supposed to modify Person in place.\n",
    "\n",
    "After running your script, the answer shown is the Person table. The driver will first compile and run your piece of code and then show the Person table. The final order of the Person table does not matter.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "| id | email            |\n",
    "|----|------------------|\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwindowSpec\u001b[39m: \u001b[32mWindowSpec\u001b[39m = org.apache.spark.sql.expressions.WindowSpec@59739ed1\n",
       "\u001b[36mtoDeleteDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val windowSpec = Window.partitionBy(col(\"email\"))\n",
    "    .orderBy(col(\"id\"))\n",
    "    \n",
    "val toDeleteDF = personDS.withColumn(\"rownum\", row_number().over(windowSpec))\n",
    "    .filter(col(\"rownum\") >= lit(2))\n",
    "    .select(col(\"id\"))\n",
    "\n",
    "toDeleteDF.show()\n",
    "// Use SQL later to delete where \"id\" in toDeleteDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.13.10)",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
